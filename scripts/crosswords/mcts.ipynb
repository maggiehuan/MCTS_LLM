{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ziyu/code/LLMs/mcts-llm\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: 7e902a91********************8562. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenai\u001b[39;00m\n\u001b[1;32m      4\u001b[0m api_key \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_KEY\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m7e902a9184ec43d688407c04f1558562\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m completions_reponse \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      7\u001b[0m   model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m   prompt\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthis message is a test\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(completions_reponse)\n",
      "File \u001b[0;32m~/anaconda3/envs/minigpt4/lib/python3.9/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/minigpt4/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/envs/minigpt4/lib/python3.9/site-packages/openai/api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    222\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/envs/minigpt4/lib/python3.9/site-packages/openai/api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    619\u001b[0m         )\n\u001b[1;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    629\u001b[0m         ),\n\u001b[1;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    631\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/minigpt4/lib/python3.9/site-packages/openai/api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    689\u001b[0m     )\n\u001b[1;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Incorrect API key provided: 7e902a91********************8562. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\", \"7e902a9184ec43d688407c04f1558562\")\n",
    "\n",
    "completions_reponse = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"this message is a test\",\n",
    ")\n",
    "print(completions_reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tot.prompts.crosswords'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtot\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcrosswords\u001b[39;00m \u001b[39mimport\u001b[39;00m propose_prompt, value_prompt\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtot\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m gpt\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtot\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtasks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcrosswords\u001b[39;00m \u001b[39mimport\u001b[39;00m MiniCrosswordsEnv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tot.prompts.crosswords'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tot.prompts.crosswords import propose_prompt, value_prompt\n",
    "from tot.models import gpt\n",
    "from tot.tasks.crosswords import MiniCrosswordsEnv\n",
    "\n",
    "env = MiniCrosswordsEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's play a 5 x 5 mini crossword, where each word should have exactly 5 letters.\n",
      "\n",
      "Current Board:\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "_____\n",
      "\n",
      "Unfilled:\n",
      "h1. An agendum; something to be done: _____\n",
      "h2. An engine: _____\n",
      "h3. Pretentious; flowery: _____\n",
      "h4. A salon; a hall: _____\n",
      "h5. To mock; to sneer: _____\n",
      "v1. To heap: _____\n",
      "v2. An Indian antelope: _____\n",
      "v3. To intend; to plan; to devise; a nettle; to guess: _____\n",
      "v4. A nozzle: _____\n",
      "v5. Desiccator; more dry: _____\n",
      "\n",
      "Filled:\n",
      "\n",
      "Changed:\n",
      "\n",
      "\n",
      "Given the current status, list all possible answers for unfilled or changed words, and your confidence levels (certain/high/medium/low), using the format \"h1. apple (medium)\". Use \"certain\" cautiously and only when you are 100% sure this is the correct word. You can list more then one possible answer for each word.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def prompt_wrap(obs):\n",
    "    return propose_prompt.format(input=obs)\n",
    "\n",
    "print(prompt_wrap(env.reset(0)))\n",
    "# print('---------')\n",
    "# print(prompt_wrap(env.step('h2. value')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "from tot.models import gpt\n",
    "\n",
    "def parse_line(input_str):\n",
    "    pattern = r'^([hv][1-5])\\. ([a-zA-Z]{5,5}) \\((certain|high|medium|low)\\).*$'\n",
    "\n",
    "    match = re.match(pattern, input_str)\n",
    "\n",
    "    if match:\n",
    "        parts = [match.group(1), match.group(2), match.group(3)]\n",
    "        return parts\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "confidence_to_value = {'certain': 1, 'high': 0.5, 'medium': 0.2, 'low': 0.1}  # TODO: ad hoc\n",
    "\n",
    "def parse_response(response):\n",
    "    lines = response.split('\\n')\n",
    "    parsed_lines = [parse_line(line) for line in lines]\n",
    "\n",
    "    parsed_lines = [(line[0].lower() + '. ' + line[1].lower(), confidence_to_value.get(line[2], 0)) for line in parsed_lines if line is not None]\n",
    "\n",
    "    return parsed_lines if len(parsed_lines) >= 1 else None\n",
    "\n",
    "\n",
    "def get_candidates_to_scores(env):\n",
    "    obs = env.render()\n",
    "    if obs in env.cache: \n",
    "        print('cache hit')\n",
    "        return env.cache[obs]\n",
    "    print('call gpt')\n",
    "    responses = gpt(prompt_wrap(obs), model='gpt-4', n=8)\n",
    "    candidates_to_scores = {}\n",
    "    for response in responses:\n",
    "        parsed_response = parse_response(response)\n",
    "        if parsed_response:\n",
    "            for candidate, score in parsed_response:\n",
    "                candidates_to_scores[candidate] = candidates_to_scores.get(candidate, 0) + score\n",
    "        # choose candiate with highest score\n",
    "    # print(sorted(candidates_to_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    env.cache[obs] = candidates_to_scores\n",
    "    return candidates_to_scores\n",
    "\n",
    "def propose_score(env, idx):\n",
    "    obs = env.reset(idx)\n",
    "    done = False\n",
    "    infos = []\n",
    "    while not done:\n",
    "        responses = gpt(prompt_wrap(obs), model='gpt-4', n=5)\n",
    "        candidates_to_scores = {}\n",
    "        for response in responses:\n",
    "            parsed_response = parse_response(response)\n",
    "            if parsed_response:\n",
    "                for candidate, score in parsed_response:\n",
    "                    candidates_to_scores[candidate] = candidates_to_scores.get(candidate, 0) + score\n",
    "        # choose candiate with highest score\n",
    "        print(sorted(candidates_to_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        if len(candidates_to_scores) == 0:\n",
    "            break\n",
    "        candidates =  sorted(candidates_to_scores, key=candidates_to_scores.get, reverse=True)\n",
    "        for candidate in candidates:\n",
    "            env_ = copy.deepcopy(env)\n",
    "            env_.step(candidate)\n",
    "            if not any(_ == 2 for _ in env_.status):\n",
    "                break\n",
    "        print(candidate)\n",
    "        # candidate = input()\n",
    "        obs, r, done, info = env.step(candidate)\n",
    "        print(obs)\n",
    "        print(env.steps, info)\n",
    "        print('-------------------\\n\\n\\n')\n",
    "        infos.append(info)\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcts(env, num_simulations, max_depth):\n",
    "    root_state = env.render()  # Initial state of the environment\n",
    "\n",
    "    # Create the root node of the MCTS tree\n",
    "    root_node = MCTSNode(state=root_state)\n",
    "\n",
    "    # MCTS lppo\n",
    "    for _ in range(num_simulations):\n",
    "        selected_node = root_node\n",
    "        while not selected_node.is_terminal() and selected_node.is_fully_expanded():\n",
    "            selected_node = selected_node.select_child()\n",
    "\n",
    "        if not selected_node.is_terminal():\n",
    "            new_state = selected_node.expand()\n",
    "            new_node = MCTSNode(state=new_state, parent=selected_node)\n",
    "            selected_node.add_child(new_node)\n",
    "            selected_node = new_node\n",
    "\n",
    "        if not selected_node.is_terminal():\n",
    "            result = selected_node.simulate(max_depth)\n",
    "        selected_node.backpropagate(result)\n",
    "\n",
    "    best_action = root_node.get_best_action()\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mcts() takes 3 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 179\u001b[0m\n\u001b[1;32m    177\u001b[0m max_depth \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m  \u001b[39m# Adjust this value as per your requirements\u001b[39;00m\n\u001b[1;32m    178\u001b[0m max_per_state \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m--> 179\u001b[0m task\u001b[39m.\u001b[39;49mmcts(actions, infos, num_simulations, max_depth, max_per_state)\n\u001b[1;32m    180\u001b[0m infoss\u001b[39m.\u001b[39mappend(infos)\n\u001b[1;32m    181\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlogs/crosswords/infoss_mcts.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fout:\n",
      "\u001b[0;31mTypeError\u001b[0m: mcts() takes 3 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, state, parent=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) == len(self.get_all_actions())\n",
    "\n",
    "    def is_terminal(self):\n",
    "        return self.state.is_done()  \n",
    "\n",
    "    def expand(self):\n",
    "        untried_actions = [a for a in self.get_all_actions() if a not in self.get_child_actions()]\n",
    "        if not untried_actions:\n",
    "            return None\n",
    "        action = random.choice(untried_actions)\n",
    "        new_state = self.state.step(action)  \n",
    "        return new_state\n",
    "\n",
    "    def get_all_actions(self):\n",
    "        return self.state.get_all_actions()  \n",
    "\n",
    "    def get_child_actions(self):\n",
    "        return [node.state for node in self.children]\n",
    "\n",
    "    def select_child(self):\n",
    "        exploration_param = 1.0\n",
    "        return max(self.children, key=lambda node: node.value / (node.visits + 1e-6) + exploration_param * (self.visits ** 0.5) / (node.visits + 1e-6))\n",
    "\n",
    "    def simulate(self, max_depth):\n",
    "        current_depth = 0\n",
    "        state = self.state\n",
    "        while not state.is_done() and current_depth < max_depth:\n",
    "            action = random.choice(state.get_all_actions())\n",
    "            state = state.step(action)  \n",
    "            current_depth += 1\n",
    "        return state.get_reward()  \n",
    "\n",
    "    def backpropagate(self, result):\n",
    "        node = self\n",
    "        while node is not None:\n",
    "            node.visits += 1\n",
    "            node.value += result\n",
    "            node = node.parent\n",
    "\n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)\n",
    "\n",
    "    def get_best_action(self):\n",
    "        best_child = max(self.children, key=lambda node: node.visits)\n",
    "        return self.get_action_to_reach_child(best_child)\n",
    "\n",
    "    def get_action_to_reach_child(self, child):\n",
    "        return [action for action in self.get_all_actions() if self.state.step(action) == child.state][0]\n",
    "\n",
    "class MiniCrosswordsMCTSTask(MiniCrosswordsTask):\n",
    "    def __init__(self, file):\n",
    "        super().__init__(file=file)\n",
    "    \n",
    "    # def reset(self, idx):\n",
    "    #     super.env.reset(idx)\n",
    "    \n",
    "    def mcts(self, num_simulations, max_depth):\n",
    "        actions = []\n",
    "        infos = []\n",
    "        num_simulations = 100\n",
    "        max_depth = 100  \n",
    "        max_per_state = 3\n",
    "        \n",
    "        root_state = self.env.render()  \n",
    "\n",
    "        # root_node = MCTSNode(state=root_state)\n",
    "\n",
    "        # for _ in range(num_simulations):\n",
    "        #     selected_node = root_node\n",
    "        #     while not selected_node.is_terminal() and selected_node.is_fully_expanded():\n",
    "        #         selected_node = selected_node.select_child()\n",
    "\n",
    "        #     if not selected_node.is_terminal():\n",
    "        #         new_state = selected_node.expand()\n",
    "        #         if new_state is not None:\n",
    "        #             new_node = MCTSNode(state=new_state, parent=selected_node)\n",
    "        #             selected_node.add_child(new_node)\n",
    "        #             selected_node = new_node\n",
    "\n",
    "        #     if not selected_node.is_terminal():\n",
    "        #         result = selected_node.simulate(max_depth)\n",
    "\n",
    "        #     selected_node.backpropagate(result)\n",
    "\n",
    "        # # Return the best action based on the most visited child node\n",
    "        # best_action = root_node.get_best_action()\n",
    "        # return best_action\n",
    "\n",
    "    def evaluate(self, x: str, y: str, n_evaluate_sample: int) -> int:\n",
    "        self.set_status(x, y)\n",
    "        assert n_evaluate_sample == 1 \n",
    "        count = {'sure': 0, 'maybe': 0, 'impossible': 0}\n",
    "        for ans, data, status in zip(self.env.ans, self.env.data, self.env.status):\n",
    "            if ans.count('_') >= 4:\n",
    "                continue\n",
    "            ans = ' '.join(ans.lower())\n",
    "            line = f'{data}: {ans}'\n",
    "            prompt = propose_prompt.format(input=line)\n",
    "            res = gpt(prompt, model='gpt-4', n=8)[0]\n",
    "            print(line)\n",
    "            print(res)\n",
    "            print()\n",
    "            proposals = self.propose_outputs_unwrap(x, y, [res], n_max_propose=-1)\n",
    "            res = proposals[0].split(' ')[1].strip()\n",
    "            if res in count:\n",
    "                count[res] += 1\n",
    "        print(count)\n",
    "        return count\n",
    "\n",
    "    def set_status(self, x: str, y: str):\n",
    "        super().set_status(x, y)\n",
    "\n",
    "    def reset(self, idx):\n",
    "        self.env.reset(idx) \n",
    "\n",
    "def mcts1(env, actions, infos, num_simulations, max_depth, max_per_state):\n",
    "    # get candidate thoughts using MCTS\n",
    "    action = mcts(env, num_simulations, max_depth)\n",
    "\n",
    "    obs, r, done, info = env.step(action)\n",
    "    r = info['r_word']\n",
    "    if len(infos) < time_limit and env.steps < 10 and not any(_ == 2 for _ in env.status):  # not violating any existing constraints\n",
    "        cnt_per_state += 1\n",
    "        if cnt_per_state > max_per_state:\n",
    "            return\n",
    "\n",
    "        count = env.prompt_status()\n",
    "        actions.append(action)\n",
    "\n",
    "        print(len(infos))\n",
    "        print(actions)\n",
    "        print(env.render_board())\n",
    "        print(info)\n",
    "        print(count)\n",
    "        if infos:\n",
    "            best = max(infos, key=lambda x: x['info']['r_word'])\n",
    "            print('best', best)\n",
    "        print('--------------')\n",
    "        print()\n",
    "\n",
    "        info = {'total_step': len(infos), 'env_step': env.steps, 'actions': actions.copy(), 'info': info, 'count': count}\n",
    "        infos.append(info)\n",
    "        if not prune or count['impossible'] < 1:  # only continue if the current status is possible\n",
    "            mcts1(env, actions, infos, num_simulations, max_depth, max_per_state)\n",
    "        actions.pop()\n",
    "    env.reset(env.idx, board=board.copy(), status=status.copy(), steps=steps)\n",
    "\n",
    "# Crosswords\n",
    "infoss = []\n",
    "for i in range(0, 100, 5):\n",
    "    task = MiniCrosswordsMCTSTask(file='/home/ziyu/code/LLMs/tot-llm/src/tot/data/crosswords/mini0505.json')  # Replace 'path_to_csv_file' with the actual file path\n",
    "    task.reset(i)\n",
    "    infos = []\n",
    "    actions = []\n",
    "    num_simulations = 100\n",
    "    max_depth = 100  # Adjust this value as per your requirements\n",
    "    max_per_state = 3\n",
    "    task.mcts(actions, infos, num_simulations, max_depth, max_per_state)\n",
    "    infoss.append(infos)\n",
    "    with open('logs/crosswords/infoss_mcts.json', 'w') as fout:\n",
    "        json.dump(infoss, fout)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
